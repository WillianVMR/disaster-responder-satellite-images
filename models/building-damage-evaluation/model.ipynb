{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class DisasterDataset(Dataset):\n",
    "    def __init__(self, input_dir, target_dir):\n",
    "        self.input_dir = input_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.filenames = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "        # Define the transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((1024, 1024)),  # Resize if not already 1024x1024\n",
    "            transforms.ToTensor(),  # Convert images to tensors\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_path = os.path.join(self.input_dir, self.filenames[idx])\n",
    "        target_path = os.path.join(self.target_dir, self.filenames[idx].replace('.png', '_target.png'))\n",
    "\n",
    "        input_image = Image.open(input_path).convert('RGB')\n",
    "        target_image = Image.open(target_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "        return input_image, target_image\n",
    "\n",
    "# Paths to data storagement\n",
    "train_input_folder_path = '/Users/willianribeiro/Documents/GitHub/disaster-responder-satellite-images/data/processed/building_identification/train/input_images'\n",
    "train_output_folder_path = '/Users/willianribeiro/Documents/GitHub/disaster-responder-satellite-images/data/processed/building_identification/train/output_images'\n",
    "\n",
    "test_input_folder_path = '/Users/willianribeiro/Documents/GitHub/disaster-responder-satellite-images/data/processed/building_identification/validation/input_images'\n",
    "test_output_folder_path = '/Users/willianribeiro/Documents/GitHub/disaster-responder-satellite-images/data/processed/building_identification/validation/output_images'\n",
    "\n",
    "dataset = DisasterDataset(train_input_folder_path, train_output_folder_path)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "val_dataset = DisasterDataset(test_input_folder_path, test_output_folder_path)\n",
    "val_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using pytorch U-net model\n",
    "\n",
    "1. Loss function: Since this is an image-to-image translation, a combination of Mean Squared Error (MSE) and a perceptual loss like VGG-based loss could be effective.\n",
    "2. Optimizer: Commonly, Adam is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        x = self.up(x)\n",
    "        # Input is CHW\n",
    "        diffY = skip_x.size()[2] - x.size()[2]\n",
    "        diffX = skip_x.size()[3] - x.size()[3]\n",
    "\n",
    "        x = F.pad(x, [diffX // 2, diffX - diffX // 2,\n",
    "                      diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://discuss.pytorch.org/t/semantic-segmentation-unet-padding/2971\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256, bilinear)\n",
    "        self.up2 = Up(512, 128, bilinear)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "# Example usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(n_channels=3, n_classes=3).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/700 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "num_epochs = 50  # Start with more epochs, let early stopping handle the cutoff\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm(loader, unit=\"batch\") as tepoch:\n",
    "        for inputs, targets in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for val_inputs, val_targets in val_loader:  # Assume you have a validation loader\n",
    "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_targets).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
